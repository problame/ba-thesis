\documentclass[12pt,a4paper]{book}
\usepackage[utf8]{inputenc}
\usepackage[inline]{enumitem}
\usepackage{parskip} % disable indentation for new paragraphs, increased margin-bottom instead
\usepackage[american,ngerman]{babel}
\usepackage{csquotes}

\usepackage{kit_style/kitthesiscover}

\usepackage[style=alphabetic]{biblatex}
\addbibresource{bib.bib}

\usepackage[%dvipdfm,
   pdfauthor={Christian Schwarz},
   pdftitle={Stage-aware Scheduling in a Library OS},
   pdfsubject={Bachelor Thesis},
   pdfkeywords={Operating Systems, Library OS, Scheduler, Cache-Affinity}
]{hyperref}

\usepackage{todonotes}
\usepackage{blindtext}

\usepackage{xparse}
\NewDocumentCommand{\sectionsubheading}{m}{%
    {\vspace{-0.7cm}\footnotesize #1 \vspace{0.25em}}\par%
}

\begin{document}
\frontmatter
\unitlength1cm
\selectlanguage{american}

\title{Stage-aware Scheduling in a Library~OS}
\author{cnad. inform. Christian Schwarz}
\thesistype{Bachelor Thesis}
\primaryreviewer{Prof.\ Dr.\ Frank Bellosa}
\advisor{M.\ Sc.\ Mathias Gottschlag}{}
\thesisbegindate{XX.\ December 2017}
\thesisenddate{XX.\ March 2018}
\maketitle

\input{kit_style/declaration}

\chapter{Abstract}
\chapter{Acknowledgments}

\mainmatter
\cleardoublepage
\phantomsection
\addcontentsline{toc}{chapter}{Contents}
\tableofcontents

\chapter{Introduction}
Network servers account for a significant percentage of contemporary application software and play a central role in today's public and corporate infrastructure.
This class of application software is primarily concerned with handling of concurrent requests.
A common design pattern is to implement a \emph{sequential} handler routine to process each request, consisting of logical stages such as protocol handling, decoding/encoding, actual business logic, etc.
Concurrency is achieved by executing the handler code per connection in separate operating system thread -- either by spawning a thread per request or by using a thread pool.
The sequential handler implementation is simple to follow for programmers and abstractions for the aforementioned threading models are available in virtually any programming language and operating system.\todo{ref}

The dominant CPUs in the server market are \texttt{x86\_64} based Intel processors targeted at the mass market.
The memory hierarchy of processors since the Haswell microarchitecture features separate 32KiB L1-d and L1-i caches and a unified 256KiB L2 cache per core, as well as a shared inclusive L3 cache that is significantly larger.\todo{ref}
%Hardware-supported operating system virtualization using KVM or Xen is commonly used to isolate applications both for simplified administration and increased security.
%Linux is the dominant guest operating system offering a familiar environment and high-performance paravirtualization drivers.
Recent large-scale profiling at Google suggests that the memory hierarchy of these commodity processors is suboptimal for the server applications they run:
the request-handlers' instruction working-set does not fit into the on-core caches, ultimately leading to pipeline stalls while the processor waits for slower L3 accesses.\cite{kanev2015profiling}

Previous work in this field includes techniques such as \emph{cohort scheduling}:
threads in the same working set are grouped into \emph{cohorts} and dispatched in series to reap the benefits of warm instruction caches.
If the shared working set fits into the on-core caches, instruction-dependent pipeline stalls are greatly reduced.
\emph{Staged Computation} brings this concept to general software architecture and \emph{staged event-driven architectures} (SEDA) generalize it further:
request processing state is encapsulated into an object which is passed through a pipeline of stages interconnected by queues.
Each stage controls the concurrency model used to perform its work, allowing cohort scheduling to be employed.
A derivative of cohort scheduling has been implemented in a research DBMS under them term \emph{STEPS}, yielding an overall speedup of $1.4$ in the TPC-C benchmark.\cite{cohort,seda,steps,harizopoulos2005staged}

Despite promising results of above research, it has found little adoption in popular database management systems such as MySQL, which uses thread-basaed concurrency (one handler per request) with a pre-spawned pool of handler threads.\cite{mysqlThreading}.
% TODO relevance of research for cohort on SMP 
% TODO next line maybe leave it out
By analyzing project development discussions, we show that the complex refactoring needed to adopt staged computation has been limiting its adoption in legacy applications TODO such as.
We present experimental work at the KIT OS group showing an approach to staged computation that requires minimal customization of existing applications:
the application develper manually defines stages and inserts one-line stage switching calls into the request handling code.
By dedicating CPU cores to stages and migrating threads between the cores on stage switch, a prototype implementation in Linux reduces instruction cache misses by X\% \todo{data}.
However, experiments show that the prototype is not work-conserving on multi-processor systems.\todo{ref}

This thesis shows that proper OS abstractions enable adoption of staged computation in legacy applications with minimal customization effort:
\begin{itemize}
    \item We analyze the design and implementation of the Linux proof-of-concept and point out why it is not work-conserving.
    \item We present OS abstractions and a user-space C++ API for applications to manually define stages and stage switching points in the request-handling code path.
    \item We present the design and implementation of a work-conserving scheduling policy that allocates CPUs to stages and migrates threads as necessary to preserve warm instruction-caches.
    \item We show that appropriate stage partitioning and switching points lead to a reduced cache miss rate of X\% in synthetic benchmarks and throughput gains of X\% in a stage-aware version of MySQL under the TPC-C benchmark.\todo{measurement}
\end{itemize}

The remainder of this thesis is structured as follows:
In Chapter~\ref{ch:relwork}, we examine preceding work in the area of staged computation.
Chapter~\ref{ch:analysis} dissects the KIT OS group's proof-of-concept implementation and points out why the design is not work conserving.
Subsequently, Chapter~\ref{ch:di} presents the design and implementation of our solution in the OSv library operating system.
Finally, we evaluate our implementation in Chapter~\ref{ch:eval}, using both synthethic microbenchmarks and the TPC-C benchmark against a stage-aware version of MySQL to show the intended cache behavior and performance improvements.

\chapter{Related Work}\label{ch:relwork}
High-performance network servers must handle requests at a degree of concurrency that exceeds the real parallelism provided by hardware in the form of CPU cores.
Much research has gone into techniques to handle this problem, further constrained by additional requirements such as predictable response times and fairness among clients.
This chapter starts with an overview of the prevalent concurrency models in the early 2000s' software landscape.
We proceed with an introduction to \emph{cohort scheduling} and \emph{staged computation} which stem from the same time period
and present an application of these proposals to a research database management system, resulting in significant performance improvements due to reduced i-cache misses and better branch prediction.

In an interim section, we analyze the applicability of the above ideas to contemporary multi-processor systems and memory hierarchies, coming to the conclusion that major redesigns are required to make them work-conserving.\todo{... for \$noun}

Recent profiling work at Google shows that typical datacenter applications' large instruction working set is still cause for major performance bottlenecks on contemporary CPUs.
We present exsiting work published under the name \emph{computation spreading} that explicitly targets SMP systems and shows how to use their memory hierarchy more effectively.
We conclude with a proof-of-concept implementation at the KIT OS group which extends computation spreading to arbitrary split points within user-level code.

\NewDocumentCommand{\relworksection}{s m m g}{%
    \section{#2}\label{#3}%
    \IfValueT{#4}{%
        \IfBooleanTF{#1}{%
            \sectionsubheading{#4}%
        }{%
            \sectionsubheading{This section summarizes \cite{#4} by \citeauthor*{#4}.}%
        }%
    }%
}%

\relworksection{Network Servers \& Concurrency}{ch:relwork:concmod}
% analysis of different htreading models with temrinology used in cohort scheduling: anderson paper
% better terminology can be foun din the SEDA paper
% see flash web server
% establish terminology like per-request threading, thread pool?
% flash web terminology => ours
% multi process / multi threaded = thread per request / thread pool
Network server software is generally concerned with receiving requests from clients over a network connection, acting upon them and returning a response.
In addition to the network I/O, disk access is very common, for example within file servers.
The request handlers are thus commonly I/O bound.\todo{proof}

I/O \emph{hardware} interfaces are asynchronous, allowing the CPU to perform useful work while the slow I/O operation completes.
However, the traditional \emph{software} abstractions exposed by the operating system are synchronous:
processes or threads that \emph{block} on I/O operations are preempted from the CPU and only resume execution once the result of the operation is available.
\citeauthor*{flashwebsrv} identify two architectures that are derived directly from this situation:
in \emph{multi-process} and \emph{multi-threaded} server architectures, request handling is encoded in a single sequential control flow with blocking I/O operations inbetween.
Concurrency is then achieved by having multiple threads (either spawned on demand or from a pre-spawned pool) execute the same request-handling function for different connections and to interleave these control flows by time-multiplexing the CPU.
Since the activity is I/O bound, this interleaving happens most often when an I/O operation blocks or at the end of the scheduler-assigned time slice.
Canonical scalability problems with this approach are the context switching overhead and the minimal amount of memory each thread requires, e.g. for its TCB and stack.
Furthermore, synchronizing request handlers on shared resources without busy waiting requires kernel supported synchronization primitives, implying syscall overhead.~\cite{flashwebsrv,c10k,andersonThreads,seda}

Asynchronous I/O interfaces are the obvious alternative:
applications initiate an operation and use polling or mechanisms such as \texttt{select} to check for completion as necessary.
Numerous mechanisms for communicating completion \emph{events} to the application have been explored and are best summarized in \cite{c10k} and \cite{seda}.
A common pattern built ontop these primitives is the \emph{event loop} or \emph{event-driven architecture}:
request processing state is not implicitly encoded in a sequential code path and the thread's state but instead explicitly defined as data.
The server consists of a loop that allocates a state object per request and a finite state machine driven by those objects.
Each thread of the server acts on one state object at a time, but does not perform blocking operations.
Instead, the operations are handed off to the kernel and the state machine switches to the next state object.
When an asynchronous I/O operation completes, the server loop picks up this event and changes the corresponding state object, which eventually triggers the state machine to perform the next logical request-handling step.~\cite{flashwebsrv,seda}

The difference becomes even clearer when assessing the use of multiple CPU cores:
\emph{multi-process} and \emph{multi-thread} delegate the task of load balancing between the cores to the OS scheduler, which faces the trade-off between cache-affinity and work-conservation.
The \emph{event-driven} approach in contrast needs only one thread per physical core, because the event loop never blocks except if no events occur, waiting for event delivery.
Since events are not bound to a thread but to the process, each event loop on any core can take over processing of new events for any state object, which results in automatic load balancing.\todo{proof}

\relworksection{Cohort Scheduling \& Staged Computation}{ch:relwork:cohort}{cohort}
In the early 2000s, \citeauthor*{cohort} investigated the cache behavior of I/O intensive online-transaction processing workloads (OLTP) in servers following the multi-process or multi-threaded architecture.
They observe\todo{time? should prev sentence be present?} high cache miss rates and instruction stalls, attributing it to the large amount of system calls made by the request handler threads:
system calls themselves have a large working set disjoint from the application code and may bring an entirely different working set into the cache when blocking and switching to another thread.

\emph{Cohort scheduling} is then proposed as a scheduling policy to dispatch threads currently sharing a working set in batches (\emph{cohorts}):
the first thread in a cohort may incur instruction / data cache misses but all successors of the same cohort benefit from a warm cache.
Naturally, the threads must yield the CPU before leaving the shared working set.
The size of a cohort represents the central tradeoff in the scheduling policy: large cohorts yield fewer amortized cache misses but cause higher response time due to progress only being made once enough threads have reached the synchronisation point required for batch dispatch.

For immediate application, systems calls are proposed as pre-existing synchronisation points.
However, to increase cache locality in arbitrary parts of an application, the authors present the concept of \emph{staged computation}:
in this programming model, instead of synchronous calls to subroutines, the request-handling stage posts asynchronous operation requests to other stages, each encapsulating a particular functionality and state.
A stage has \emph{scheduling autonomy}, which means it is free to choose the concurrency model that suits the type of provided operation best --- cohort scheduling being just one of several options.

\relworksection{Staged Event-Driven Architecture}{ch:relwork:seda}{seda}
% difference to staged computation: scalabiltiy, graceful degradation, no focus on cache locality
% formulation: control boundary, page 5 bottem right
% ease-of-use for application developers
% ref to cohort scheduling page 6 up left: example for auto-tuning of batrching factor (cohort size)
% give outlook on OS support for SEDA: specialized OS, no need for transparent resource virtualization as provided by threads
\emph{Staged event-driven architecture} (SEDA) is a software architecture generalizing the idea of staged computation.
However, the primary motivation for its inception was the construction of network services whose performance should degrade linearly under an increasing number of concurrent requests.
SEDA provides a framework for application developers to implement an event-driven server by only providing event handlers.
Each event handler represents a \emph{stage} and is invoked by its \emph{stage controller}, which executes the event handler on the stage's thread pool with input from the stage's \emph{incoming event queue}.
The event handler can enqueue additional work into other stages' queues, but cannot call into their code directly, enforcing an explicit boundary in the application's control flow.
The stage controller works in a feedback loop to ensure a stage's performance requirements, for example by adjusting the thread pool / \emph{cohort size} in order to meet a certain response time goal for event handlers.%
\footnote{SEDA terminology for a resource controller implementing cohort scheduling is \textquote{batching controller}.}
Stage controllers can coordinate to avoid resource overcommitment.
It cannot be stressed enough that the customizability of the stage controller is central to the whole concept of SEDA:
application developers are given control over the degree of concurrency and can address performance of code modules through custom stage controllers.

The authors evaluate SEDA by implementing an HTTP server and measuring the performance metrics \emph{total concurrent throughput} and \emph{response time} over a varying number of concurrent clients.
Additionally, they measure the amount of requests each client completes, defining an equal distribution of service among clients as ideally \emph{fair}. %Jain fairness factor
The results show 16--20\% higher throughput at little-varying response times and high fairness among clients in comparison to the the multi-threaded Apache web server. % leave out Flash, not interesting to us
The authors' explanation for these results is that the SEDA server queues all requests \emph{inside} the application while the multi-threaded Apache web server will not accept new client connections when reaching the maximum size of its thread pool, causing exponentially increasing response times for non-accepted clients due to exponential backoff algorithm employed in TCP.

%Notably, the authors suggest operating systems should support SEDA natively, emphasized by their need to implement asynchronous I/O abstractions for their resource controllers.\todo{leave out?}

\relworksection{STEPS}{ch:relwork:steps}{steps}
% split up dbms into stages by operators
% formulate the scheduling trade-off we found, too: response time vs cache efficiencey
% => make clear we handle this in our scheduling policy by balancing queue lenghts (i.e. adding more CPUs as necessary)
% in principle cohort scheduling, but with 'production line mode' in mind, this is (not exactly what we do, in theory stages can be entered again...)
% come to conlcusion that staged system must have dedicated CPUs per stage (p10, 5.3 intro)
% use ults to switch between threads in a cohort (3.3.1):
% p8 rechts oben: threads executing same db operation are grouped into teams of so called S-threads, linked in doubly-linked list. at ctx, switch to next entry in linked list => fast, high hit rates
%                 when an S-thread blocks (yields the CPU), it leaves that team (stray thread)
% p9: data-dependnet teams where overalp between S-threads is not big enough
% p9: trick for blocking / yielding threads:
%       1. intercept block events, remove blocking thread from team list, hint to "regular" (=OS/thread package) scheduler next thread in team-list should be run, continue blocking, will schedule team-list memeber
%       2. (rechts unten): nur wenn man keine mutex hält wird gescheduled, sonst nicht, weil dann die ganzen anderen thread snicht weiterkommen
%       => THEY HAD THE SAME PROBLME WE HAD!
%       => section 4.1.4 reads like they did all modifications to the DB, nothing reusable in other applications

% paper review:
% 3.1 "Most commercial DBMS involve a light-weight mechanism to pass on CPU control (Shore uses user-level threads)." (trifft bspw. auf MySQL nicht zu)
The \emph{Synchronized Transactions through Explicit Processor Scheduling} (STEPS) project implements a derivative of cohort scheduling in the research database system SHORE.~\cite{shore}

The authors replicate the observation of \cite{cohort} for database systems: the execution flow exhibits low spatial locality and do not fit into the CPU caches, leading to pipeline stalls and a high amount of mispredicted branches.
In contrast to cohort scheduling, STEPS groups threads handling a transaction into \emph{team lists} organized by high-level operation (e.g. \texttt{INSERT}, \texttt{UPDATE}, etc).
The code path of the operation then contains migration points (\textit{\texttt{CTX} calls}) at which the currently executing thread yields control to the next member of its team list, which amounts to user-mode round-robin scheduling within a team.
Migration points are placed such that the working set between two of them fits into the CPU's L1 i-cache.
The idea of cohort scheduling is found in the step-wise progression over the long code path of the high-level operation:
we imagine three migration points $A$, $B$ and $C$ and a team of ten threads executing between $A$ and $B$, operating on a fully utilized warm i-cache.
After the last thread reaches $B$, the team moves forward to the next step  \textit{$B$ to $C$}.
While the first thread executing this step will incur compulsory \todo{capacity} i-cache misses, the remaining nine threads benefit from a warm i-cache and branch predictiors.

Blocking threads or threads aborting the transaction, would fall behind in this trickle-down schema and thus replace their ex-team members' cache state.
The solution is to remove threads aborting their transaction or blocking for synchronization and I/O from the team list and track them as \emph{stray threads}.
The publications on STEPS describe that the thread package in SHORE required modification to support this schema, but falls short on implementation details.
For this thesis though, blocking is a highly relevant topic because it is a major problem in the proof of concept implementation at KIT OS group (see Section~\ref{ch:relwork:kitpoc} and Chapter~\ref{ch:analysis}).
By examining the 6.0 release of SHORE which was published after STEPS, we can identify a base class for all DBMS threads called \texttt{sthread\_t} which provides wrappers for basic blocking I/O functions such as the \texttt{read} or \texttt{write} syscalls.
%Under the assumption that these abstractions existed in the version of SHORE that STEPS is based on, we can imagine that they were used to get notified about potentially blocking I/O requests by a thread in order to remove it from the team list.
%However, such an approach would not be able to differentiate between an actual block vs. non-blocking retrieval from the page cache and thus cause avoidable collateral damage to the thread requesting I/O.
%On the other hand, the code footprint of the I/O path could evict the team's working set from the cache --- an argument in favor of indistinguished removal from the team if I/O is pending.
Under the assumption that these abstractions existed in the version of SHORE that STEPS was based on, we are confident that the STEPS authors used these wrappers to get notified about potentially blocking I/O requests by a thread in order to remove it from its team list.~\cite{shoreRelease}

Regardless of the exact implementation details, we are not convinced that the adoption of STEPS in arbitrary applications requires as few code modifications as claimed by the authors:
SHORE already comes with a custom thread abstraction around pthreads and centralized wrappers around blocking I/O.
In contrast, arbitrary applications will use the pthreads and C standard libraries directly, in which case
\emph{either} substantial modifications to the code base are necessary to establish a situation as found in SHORE
\emph{or} runtime-patching via \texttt{LD\_PRELOAD} would be required.

Finally, \citeauthor*{steps} mentions that STEPS gives a hint to the OS scheduler when a thread blocks in order schedule the next regular team member and maximize the usage of the already warm cache.
The downside of this approach is that substantial \todo{find proof} code paths in the operating system are not covered by STEPS at all
--- stray threads will continue to compete for CPU time while they are not blocked.
An OS scheduler unaware of the team lists may thus schedule a stray threads in the midst of a team, evicting its cache state inadvertently.

\relworksection{Interim: Applicability to SMP Systems}{ch:relwork:cohortsmp}
The related work we presented so far was evaluated on a single-core machine, with adaption to multi-core systems not mentioned at all or staying theoretical.~\cite{steps,harizopoulos2003case}\\
However, we argue that the designs of cohort scheduling and STEPS must be re-examined given the ubiquity of multi-processor systems.

\subsection{Private Caches}
% All numbers by example: My Haswell against STEPS CPU
% L1 and L2 are now oncore, L3 is shared (and inclusive on Intel)
% In contrast: early 2000s processors were single-core, L1 and L2
% capacities of L2 now what we have in L2 on-core
% today's L1 is split 32KiB i- and d-cache, early 2000s L1 64KiB (shared / unified?)  
% proof that today's chips are multicore: begin sodaspr
The single-core machines used in the evaluations have a small L1 cache and a larger, higher-latency L2 cache.
However, contemporary symmetric multi-processors feature \emph{private} L1 and L2 caches and a \emph{shared} L3 cache.
Access latency to the L3 cache is orders of magnitude higher than to the private caches. \todo{proof}

Due to this discrepancy, an adaption of cohort scheduling and STEPS to multiprocessors must target the private caches in order to achieve similar speedups.
More importantly, the existence of private caches per hardware thread introduces the problem and opportunity to allocate this resource:
in terms of STEPS, multiple steps can now be spread over different cores instead of time-multiplexing the single available L1 i-cache.
We come back to this observation when assessing \emph{computation spreading} in Section~\ref{ch:relwork:compspr}.

\subsection{Real Parallelism}
We recall from Section~\ref{ch:relwork:cohort} that the scheduling tradeoff in both approaches consists of reduced cache misses vs. increased response time due to \emph{batching}.
However, the real parallelism available on SMP systems brings with it another scheduling dimension that STEPS and cohort scheduling do not address:
to utilize the available hardware threads, load must be balanced among them to maximize resource utilization.
This goal surpasses response time since as the primary scheduling goal since the the service provider generally pays for the number of cores, not their actual usage.\todo{well actually... cpu time accounting}
All relevant operating systems abstract CPU cores through threads, and the OS scheduler implements load balancing by migrating threads between cores, often directed by metrics such as the length of each core's the ready queue (\emph{load}). \todo{proof}

Cohort scheduling and STEPS were not designed for this situation because
%both solutions include \emph{batching} as a central element, either at the syscall boundary or at arbitrary points in the application code (see Section~\ref{ch:relwork:cohort}~and~\ref{ch:relwork:steps}).
\textbf{batching is inherently not work-conserving on SMP systems}:
it is the core idea of these approaches that only one thread in a cohort or team can run at any given time in order to keep the i-cache filled with the current step's code.
Furthermore, STEPS and the OS scheduler work actively against each other because the latter will see all threads in a cohort as runnable and actively spread the cohort over all (presumably idle) cores,
destroying all promised performance benefits.

An easy fix would be a mechanism for STEPS to influence the load balancing in order to favor threads of the same team to stay on the same core, e.g. through thread pinning.
This is obviously not work-conserving on multi-core systems, requiring additional mechanisms and communication between STEPS and the OS to fix it.

We observe that a pure user-space approach is inapplicable on contemporary operating systems, acknowledging the foresight of the SEDA publication that proposes OS support.
\todo{affinity scheudling --- just ignore it?}

\relworksection{Profiling Datacenter Applications}{ch:relwork:profiling}{kanev2015profiling}
% alread shown that it's a problem in DBMSs, see 'affinity scheduling harizopoulos introduction' and the Steps paper (p3)
% dominant are the d-cache misses, but i-cache misses are also significantly higher than usual 
% growth of i-cache working set over time (speculation: due to product development) => worsens situation
% => stall cycles, X %
% memory latency, not band width
While the micro-architectural performance problems caused by large i-cache footprints were already observed in \cite{cohort} and \cite{steps} in the early 2000s,
recent profiling work at Google shows that computer architecture is still not able to satisfy the requirements of contemporary scale-out applications.
However, the potential performance gains and cost savings due to improved cache behavior become relevant at scale, motivating a re-assessment of the situation.

\citeauthor*{kanev2015profiling} identify significantly higher amount of pipeline stalls in real-world datacenter software when compared to the SPEC CPU2006 benchmark.
Apart from the overhead associated with the distributed software architecture (RPC, serialization, etc) % datacenter tax
a large cache footprint (both code and data) reveals the major performance bottleneck on today's processors for typical datacenter applications:
the authors observe 60\% of $\mu$op slots to be backend-bound, and 15 -- 30\% to be front-end bound, which leads to heavy under-utilization of the CPU cores' available resources at a micro-architectural level.
In particular, the authors find that more than 5\% of cycles are wasted due to empty front-end buffers, which is attributed to instruction read misses in the private L2 cache, resulting in slow accesses to the CPU's shared last level cache.
Additionally, the observations confirm that applications under active development grow in their instruction working set, worsening the situation.%
\footnote{The authors present examplary results of up to 27\% per year.}
The authors do not explicitly investigate why datacenter applications have such large instruction working sets but mentioning \textquote{lukewarm} code and static linking as possible contributors.

In comparison to the work published a decade earlier (see Section~\ref{ch:relwork:cohort}~and~\ref{ch:relwork:steps}),
\citeauthor*{kanev2015profiling} focus on computer-architecture and only pay brief attention to the operating system:
approximately 20\% of CPU time is spent in the kernel with more than 5\% of CPU time in the scheduler alone.
Additionally, it is noteworthy that a 90-th percentile of the observed machines handles more than 4500 concurrent threads.
Given that at least a portion of these threads will be part of network servers, the question of the concurrency model thus stays very relevant although not explicitly stated as a source of performance-optimizations.

\relworksection{Computation Spreading}{ch:relwork:compspr}{compspr}
% TODO: related and actually more relevant to our approach: Computation Spreading
% => term: destructive interference
% => migration for syscall => split i-working set (naturally) + little communication (because sendfile)
\emph{Computation spreading} (CSP) in its most general form is a technique to cache the instruction working set of a process more effectively on SMP systems which commonly feature private caches per core.
\citeauthor*{compspr} show that this cache hierarchy leads to rendundantly stored code.
For example, in a traditional thread-based concurrency model, each request-handling thread may interact with the file system through system calls which leads to redundantly stored file system code being in each core's private cache.
The authors present a solution that temporarily assigns CPU cores to either execute OS code or user-level code, exploring and evaluating several allocation policies.
Threads entering or leaving the OS synchronously through syscalls, exceptions or page faults are then migrated between compatible cores.

The implementation is based on hardware virtualization allowing for interception of above kernel entries without modifying the (guest) OS or user-space as well as a hardware-provided thread migration mechanism.
Naturally, this concept requires control or at least cooperation of the software executing in hypervisor mode.

The authors evaluate their solution using full-system simulation of an eight-core machine with private L1 and L2 cache and a shared L3 cache.
Server applications, which are of primary interest for this thesis, exhibit 25 -- 58\% fewer L2 instruction misses and 9 -- 25\% improved branch misprediction rates.
Data read misses are less acffected, with only 13 -- 19\% decrease for Apache web server and an OLTP application.\todo{leave sentence out?}
The approach leads to a speed-up of up to 20\% in Apache and 9.4\% in OLTP measured by the total runtime of the full-system simulation.

Despite the very specific implementation, the general concept presented by the authors is more general and not limited to the syscall interface, suggesting arbitrary migration points and corresponding core allocation policies.

\relworksection{Protoype in Linux at KIT OS Group}{ch:relwork:kitpoc}
%TODO this section should coin the term stage-aware scheduling, otherwise, we need to do that before the analysis --- rename section with stage-aware in title
A Linux-based proof of concept implementation at the KIT OS group combines the idea of staged execution, cohort
scheduling and computation spreading into a C API that allows for intuitive conversion of existing code bases to
staged execution:
The developer manually identifies stages and inserts library calls into application code for switching the current stage.
Each CPU core is assigned one or more stages and each time a thread switches to a new stage, it is migrated to a core
assigned to that stage.

It is obvious that a fast thread migration mechanism is required for this technique to succeed
--- otherwise, the performance benefits of always-warm caches per stage are destroyed by the migration time.
%\newcommand{\setaffinity}{\texttt{sched\_setaffinity(2)}}
The Linux built-in facility for this purpose, \texttt{sched\_setaffinity}, is impractical because it uses
expensive inter-processor-interrupts to implement this syscall, resulting in $9\mu s$ -- $14\mu s$ of migration time.~\cite{sodaspr}% TODO check on IPs 
As a consequence, thread migration was implemented in user-space:
for each user-level thread (ULT), there still exists exists a kernel-level thread (KLT),
but KLTs are pinned once to a specific core and stage.
ULTs run on a KLT of the stage they are currently in and migrate to a different KLT when switchting stages.
TODO threading model required
TODO automatic -> mathias' paper, when it's published

When a ULT makes a call to switch stages, its context is saved and enqueued into the next stage's ready queue.
The originating KLT now waits for new ULTs on its own stage's incoming migrations queue.
If it is empty, the KLT makes a blocking syscall \texttt{TODO\_dequeue\_syscall} to a kernel component to wait for incoming migrations.
The kernel component must ensure that there is always one KLT per core either doing work or actively dequeuing ULTs in order to utilize the CPU.
This is implemented by a callback from the Linux scheduler that informs the kernel component of task state changes.
For example, if the currently dequeuing KLT $K_1$ executes a ULT, and that ULT blocks on a mutex, $K_1$ switches to \texttt{TASK\_INTERRUPTIBLE}.
The kernel component must wake up another KLT $K_2$ that is currently waiting for incoming migrations of that stage on that core.
Otherwise, the core does not perform any work (for the application) until $K_1$ aquires the mutex and switches back to \texttt{TASK\_RUNNING}

TODO results with single threaded execution, show that it works, reduced cache misses and performance gains are visisble.

However, multithreaded workloads on an SMP system exhibit visible CPU underutilization.
Imagine a system with $\frac{\#cores}{stage} > 1$ and a ULT $U_1$ in stage $S$ executing on a KLT $K_1$.
As described above, $K_1$ is pinned to core $C_1$.
When $U_1$ performs a blocking syscall, for example waiting to aquire a mutex via \texttt{pthread\_mutex\_lock}, $K_1$ blocks and becomes \texttt{TASK\_INTERRUPTIBLE}.
The Linux scheduler now dispatches another task $T$ on $C_1$ to maximize CPU utilization.
(Note that $T$ is not a $K_i$ of our application. In fact, all $K_i$ are blocked in \texttt{TODO\_dequeue\_syscall}.)
When $K_1$ finally aquires the mutex and is \texttt{TASK\_RUNNING} again, it can still only be dispatched to $C_1$ due to pinning which is necessary for user-level thread migration.
However, $C_1$ will be executing $T$, not $K_1$ which is put into $C_1$'s ready queue instead.
The misery at this point is that there may exist \textbf{another} CPU $C_2$ where a KLT $K_2$ is dequeuing ULTs in the \textbf{same stage} $S$:
if $K_2$ does not have any ULTs to execute, $K_1$ should be migrated to $C_2$ immediately when it is woken up and continue execution there, benefiting from the warm on-core caches.
But the implementation only performs thread migration when a ULT calls the stage switching API.
There is no mechanism in place to save $K_1$'s state and enqueue it to $K_2$'s incoming migration queue on wake-up.
(One might assume it is possible to enqueue $U_1$ to $K_2$ since we saved its register state on kernel entry via \texttt{pthread\_mutex\_lock}:
this is not possible because there might still be kernel code that needs to run after the mutex is aquired, before returning to $U_1$.)

TODO UML sequence diagram visualizing the pathological case described above.

\chapter{Analysis}\label{ch:analysis}
Related work has demonstrated that spreading the working set of an application over multiple cores yields lower on-core cache miss rates.
The proof-of-concept implementation at the KIT OS group furthermore shows that large-scale refactoring of existing code bases is not necessarily required to spread the working set:
given an application with a suitable threading model, patches of a few lines are sufficient to reduce the cache-miss rate by TODO.

Nonetheless, we identify several fundamental problems in the approach taken by the proof-of-concept implementation:
the requirement for fast thread migration drove the design toward a user-space solution which decouples the threads known by the application (ULTs) from the threads known by the kernel (KLTs).
However, the kernel scheduler still only handles KLTs and assumes a 1:1 threading model, which leads to an ambiguous role of KLTs in the proof-of-concept:
when switching between stages, the user space thread migration code views KLTs as the CPUs they are pinned to.
But when a ULT running on a KLT interacts with the Linux kernel, the kernel sees a normal \texttt{task\_struct} and continues to assume the 1:1 threading model where tasks can just block.
The proof-of-concept works around this schizophreny by introducing a callback from the scheduler to react to blocking KLTs, but fails to handle asynchronous events like wake-ups.
The latter leads to the observed pathological behavior on multi-core systems.

We conclude that the proof-of-concept does not model the situation correctly: the association of stages and CPU cores is piggybacked onto the KLTs using \texttt{sched\_setaffinity}.
Instead, we prpose that stages must be modeled explicitly in the kernel and be separate entities, beneath CPUs and threads:
\begin{itemize}%
    \item The associaton of stages and CPU cores must be represented explicity.
    \item Threads must carry the information in which stage they are executing.
    \item The scheduler must honor this information by scheduling threads onto cores that are associated with their respective stage.
    \item The scheduler must trade off the potential gains of always-warm caches against existing scheduling goals such as resource utilization, fairness and response time.
\end{itemize}%
This reverts the complex situation of ULTs and KLTs to a simple 1:1 threading model, removing the special-case of blocking kernel activity.

The remainder of this thesis presents our design as well as its implementation in the OSv library operating system and its evaluation using several microbenchmarks and the TPC-C benchmark against the MySQL database management system.

\chapter{Design \& Implementation}
In this chapter, we present the design and implementation of our solution to the proposal in Chapter~\ref{ch:analysis}.
Our design is limited to server applications with high instruction cache footprint and a thread-pool or thread-per-request threading model:
related work has shown that high instruction cache miss rates are a performance bottleneck in scale-out applications
and demonstrated lower cache miss rates in the MySQL database server, which follows the thread-per-request model with a pool of pre-spawned threads.
We re-use their tested stage definitions in MySQL \todo{KIT OS group} and focus on the implementation of a work-conserving scheduling policy.
\cite{kanev2015profiling,mysqlThreading}\todo{KIT OS group}

The limited compatibility with threading models is not a major limitation of our design:
our approach aims at reaping the benefits of staged computation in legacy applications that cannot be easily refactored to a staged-computation model.
Thread-pools and per-request threads were the dominant threading model in the last TODO years and are in fact still used in new applications today.
We acknowledge that more recent runtimes with event-driven concurrency or M:N scheduling models are incompatible with our approach, unless programmers manually construct the supported threading models on top of them.\todo{proof}\todo{discussion section?}

The following enumeration provides an overview of our contributions:
\begin{enumerate}
    \item We implement our solution in the OSv library operating system, which provides Linux-ABI compatibility and runs on Linux/KVM hypervisor.~(Section~\ref{ch:di:osv})
    \item We remove the existing load balancing and fairness mechanisms from OSv's scheduling policy, replacing it with non-preemptive round-robin per CPU core.~(Section~\ref{ch:di:rmsched})
    \item We add a user-space API to define stages and switch between them.~(Section~\ref{ch:di:api})
    \item We implement a fast thread migration mechanism that does not use inter-processor interrupts.~(Section~\ref{ch:di:mig})
    \item On wake-up of a thread, we load-balance between the CPU cores of its current stage, using thread migration as necessary.~(Section~\ref{ch:di:workcons})
    \item We add an allocation policy for CPUs to stages that maximizes throughput by removing bottleneck stages.~(Section~\ref{ch:di:pol})
\end{enumerate}
Each section starts with the reasoning behind our design followed by our implementation strategy in OSv as well as the refactorings it required.
We refer to the relevant commits in the Git repository of our modified version of OSv as appropriate.

\section{The OSv Library Operating System}\label{ch:di:osv}
% what is osv, how does it work:
%       libc + linux ABI => unmodified, existign software runs (mention ports), etc
%       compact scheduler implementation => see below
% why OSv for this thesis:
%    small kernel / scheudler => implementation complexity
%    virtual machines         => Linux scheduler for fairness goals, our OSv scheduler can focus on optimal stage-aware scheduling
%
% Design Deficiencies:
%           While offloading almost all other design goals to Linux scheduler makes implementation considerably easier,
%           it is by far not optimal: like all nested dynamic systems, the Linux scheduler and our scheduler can inadvertently work against each other
%           => for simplicitly, we limit our design to a solution where cores are dedicated to the OSv VM

\begin{itemize}
    \item Concept of a library OS: vm = app, no user-kernel-boundary => performance promise
    \item Linux ABI-compat
    \item Moderately sized C++ codebase, compact scheduler implementation
\end{itemize}
Idea: OSv allows for easier replacement of the entire scheduling policy and and thread migration mechanisms.
By the nature of a library OS, the changes are kept application-local, leaving fairness considerations up to the hypervisor scheduler (Linux / KVM in our case).

\subsection{The OSv Scheduler}
This section highlights implementation details of the upstream OSv scheduler that are required to understand the framework in which our implementation was done.
For details, we refer to the TODO paper.
\begin{itemize}
    \item runqueues + idle thread
    \item thread pinning => percpu threads
    \item thread migration mechanism
    \item thread states => state diagram
\end{itemize}

\section{Removal of OSv Scheduling Policy}\label{ch:di:rmsched}
% remove load balancing + fairness (cpu time, priority queue / sorted list? )
% stub out pthreads_sched_setaffinity_np + remove dead thread migration code (why did we do that?)
% implement simple CPU-local non-preeemptive round-robin: non-preemptive is ok because handlers will finish and is good for cache performance (see stagedExecutionDBMS, p5 up left, p10 5.1.2)
% describe outcome:
%   cpu-local runqueues, round robin, non-preemption (OKish because threads always finish, while not really fair...)
%   existing thread migration mechanism still required by percpu threads, but only on startup
%   idle threads are always runnable, dequeue incoming migrations per CPU
%   
\blindtext
\section{User-Space API}\label{ch:di:api}
% Define stages as objects in the kernel
% User-space can enqueue itself to a stage, will be migrated to one of the cores it is assigned to
% Stack structure for implementation comfort, describe idea behind stack structure & guards -> spans, like in web browser
\blindtext
\section{Fast Thread Migration}\label{ch:di:mig}
% Motivation: Stage switch requires migration to other core
% Existing migration mechanism is based on IPIs, is too slow => results, maybe also compare to Linux => adjust hopper
% Must be fast, because of opportunity cost calculation:
%   -> for single client:       cold cache and no migration vs warm cache but migration times
%   -> for T > #cpus clients: ???
% Design:
%       stage migration and hence migration to other CPU core is synchronous to program flow
%       MPSC queue per CPU, for incoming stage migrations
%       on stage switch, evaluate CPU allocation policy, pick target CPU, enqueue into stagesched_incoming
%       target CPU dequeues from stagesched_incoming in idle thread -> mention potential optimization: mwait
%       Measurement results: why not here??? specifically if we discuss mwait?
% Implementation:
%       a stage switch means scheduling out on the source CPU, being migrated to the target CPU and resuming execution there
%       per-cpu MPSC queue that contains pointers to TCBs of threads in stage migration
%       cpu idle thread dequeues from mpsc
%       additional thread states for thread in stage migration
%       critical race condition:  only after scheduling out on the source CPU must execution continue on the target CPU,
%           but thread puts itself into the MPSC queue while it is still executing
%           => augment the thread-switching routine (which is the last code the migrating thread execute on the source CPU) to atomically
%              switch the state of the thread from stagemig_run to stagemig_sto
%           => target CPU only dequeues those threads that are in stagemig_sto to its runqueue, others are still executing code on the source CPU
% => State diagram with changes made in this step
\blindtext
\section{Work Conservation}\label{ch:di:workcons}
% Motivation: with thread migration mechanism in place, we are faster than the Linux solution but not still work conserving
%             when a thread is woken up, it will be enqueued inot the runqueue of the CPU where it went to sleep
%             if that CPU is already executing a thread but another one of the same stage is idle, we do not use the available resources efficiently
%             the woken thread should be migrated to the idle CPU and immediately resume execution there
%             important: this is not stage switching, but also needs to evaluate the CPU allocation policy (forward ref!)
% Design:
%             on wakeup, evaluate the CPU allocation policy and migrate the woken thread to that CPU using the normal thread migration mechanism
% Implementation:
%             wakup is asynchronous, e.g. when from a timer 
%             we must only migrate a thread when it is stopped, otherwise it's already running and does not need to be woken up
%             but there are states in which the thread is being woken up while going to sleep
%             => cleanly encode running vs stopped in the thread state
%               => more difficult than expected
%               => extend post-switch-to mechanism with appropriate state transitions
%               => TODO see git commit
%               => state diagram with changes made in this step
%             Afterwards, just use the existing thread migration mechanism TODO check if we can go without the IPIs
\blindtext

\section{Stage-Aware Scheduling Policy}\label{ch:di:pol}
% TODO read harizopoulos, affninity scheduligng in staged servers and make sure we are not in stark contrast to their results... one argument for that could be that they focus on single CPUs and only have stiefmütterlich solution for mulitple CPUs (see section 6)
% Difference to STEPS: we target multi- to manycore go conceptually, 

% Motivation: bottleneck stages = stages with more-than-avergage threads in the ready queues of processors assigned to these stages
%             natural observation: stage needs more CPU time / cores in our case
%             alterantive formulation (is that correct?):
%                   the time it takes from begin of stage switch to first dispatch on CPU in new stage should be equal among all stages
%                   => predictable latencies
% Design:
%            among all stages, equalize the number of threads that are currently executing / enqueued in each of them
%            why?
%                   => obvious that this eleminiates bottleneck stages
%                   => idea: in systems with #concurrent_requests > #cpus, 
%                   => TODO check queueing theory, would be nice to have some actual formulas here ;)
%
%            equalizing by giving a busy stage more CPUs / taking CPUs from a non-busy stage
%            and between the CPUs assigned to a stage, arbitrate by runqueue length

%              => analogy of requests to be handled = water in pipes...?
%              => a CPU will still have threads from the previous stage in the runqueue: doesn't matter because...
%                    ...they will be migrated off this CPU as soon as they schedule out / block on IO
%                    ...round robin ensures the caches are warm for the remaining threads in the old stage
%                    ... we know the requests will finish processing eventually

% Design Deficiencies:
%              if the runqueues are long, this system reacts slowy
%                   => would need to migrate runqueue threads off-cpu 
%                       => could do this lazily in schedule()
%
% Implementation:
%               track c_in per stage in atomic counters
%               ... see patch, fairly boring
%               max_stages, snapshot runqueue counters 
\blindtext

\chapter{Evaluation}\label{ch:eval}
In this section, we present the evaluation of our design.
We present our hardware setup and describe how we verify the supposed effects of our design decisions using various benchmarks.
This section is very closely related to the previous section.
\section{Evaluation Setup}
\section{Fast Thread Migration}
\section{Work Conservation}
\section{Stage-Aware Scheduling Policy}
MySQL Migration points:
    manual: why should one use relational operatos? comparibility with staged DBMS paper
    with mathias' simulation: because it's better, show the advantage => check on mathias' paper once it is published
% alread shown that it's a problem in DBMSs, see 'affinity scheduling harizopoulos introduction and page 4'
% TODO compare I-cache missesa and mispred branches with Steps and compspr, who claim that, see steps, p2 rechts oben => 30 warehouses!
% steps paper 4.2 intro explains TPC-C
% mentio nwe run on ramdisk, does I/O actually block? how often? 
% der steps graph (figure 7) sieht ziemlich aehnlich aus
\chapter{Conclusion}\label{ch:concl}
\blindtext
\section{Future Work}
\begin{itemize}
    \item at ITEC OS Group: automatic profiling \& finding of migration points.
    \item auto-evaluating scheduler: measure if stage migrations actually make sense by computing a break-even point and continously measuring the result of scheduling decisions using performance conuters.
    \item NUMA / SMT-aware Scheduling Policy
    \item MWAIT
    \item Backpressure (like in SEDA / staged DBMS) to our design? does that make sense?
\end{itemize}

\backmatter

\chapter{Appendix}
\blindtext
\begin{itemize}
    \item Source code and commit history of our modified version of OSv
    \item Source code and commit history of our modified version of MySQL 5.6
    \item Source code and commit history of our microbenchmarks and measurement scripts
\end{itemize}

\cleardoublepage
\phantomsection
\addcontentsline{toc}{chapter}{Bibliography}
\printbibliography

\end{document}
