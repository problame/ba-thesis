\documentclass[12pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[inline]{enumitem}
\usepackage{parskip} % disable indentation for new paragraphs, increased margin-bottom instead
\usepackage[english]{babel}

\usepackage[style=alphabetic]{biblatex}
\addbibresource{bib.bib}

\usepackage{hyperref}

\usepackage{todonotes}

\title{{\large Bachelor Thesis}\\Stage-aware Scheduling in a Library OS}
\author{Christian Schwarz}
\date{TODO December 2017 -- TODO March 2018}

\begin{document}

\maketitle

\clearpage

\section{Abstract}

\clearpage

\tableofcontents

\clearpage

\section{Introduction}

\section{Related Work}
We explain motivation and related work in the field of cache-aware scheduling policies / application architecture.
This will borrow heaviliy from the expose.

\subsection{Profiling at Google (Kanev et al)}
\subsection{Memory Hierarchy on Contemporary Server-Class CPUs}
\subsection{SEDA}
\subsection{Software Data Spreading}
\subsection{Protoype in Linux at KIT OS Group}
A Linux-based proof of concept implementation at the KIT OS group combines the idea of staged execution, cohort
scheduling and software data spreading into a C API that allows for intuitive conversion of existing code bases to
staged execution:
The developer manually identifies stages and inserts library calls into application code for switching the current stage.
Each CPU core is assigned one or more stages and each time a thread switches to a new stage, it is migrated to a core
assigned to that stage.

It is obvious that a fast thread migration mechanism is required for this technique to succeed
--- otherwise, the performance benefits of always-warm caches per stage are destroyed by the migration time.
%\newcommand{\setaffinity}{\texttt{sched\_setaffinity(2)}}
The Linux built-in facility for this purpose, \texttt{sched\_setaffinity}, is impractical because it uses
expensive inter-processor-interrupts to implement this syscall, resulting in $9\mu s$ -- $14\mu s$ of migration time.~\cite{sodaspr}% TODO check on IPs 
As a consequence, thread migration was implemented in user-space:
for each user-level thread (ULT), there still exists exists a kernel-level thread (KLT),
but KLTs are pinned once to a specific core and stage.
ULTs run on a KLT of the stage they are currently in and migrate to a different KLT when switchting stages.

When a ULT makes a call to switch stages, its context is saved and enqueued into the next stage's ready queue.
The originating KLT now waits for new ULTs on its own stage's incoming migrations queue.
If it is empty, the KLT makes a blocking syscall \texttt{TODO\_dequeue\_syscall} to a kernel component to wait for incoming migrations.
The kernel component must ensure that there is always one KLT per core either doing work or actively dequeuing ULTs in order to utilize the CPU.
This is implemented by a callback from the Linux scheduler that informs the kernel component of task state changes.
For example, if the currently dequeuing KLT $K_1$ executes a ULT, and that ULT blocks on a mutex, $K_1$ switches to \texttt{TASK\_INTERRUPTIBLE}.
The kernel component must wake up another KLT $K_2$ that is currently waiting for incoming migrations of that stage on that core.
Otherwise, the core does not perform any work (for the application) until $K_1$ aquires the mutex and switches back to \texttt{TASK\_RUNNING}

TODO results with single threaded execution, show that it works, reduced cache misses and performance gains are visisble.

However, multithreaded workloads on an SMP system exhibit visible CPU underutilization.
Imagine a system with $\frac{\#cores}{stage} > 1$ and a ULT $U_1$ in stage $S$ executing on a KLT $K_1$.
As described above, $K_1$ is pinned to core $C_1$.
When $U_1$ performs a blocking syscall, for example waiting to aquire a mutex via \texttt{pthread\_mutex\_lock}, $K_1$ blocks and becomes \texttt{TASK\_INTERRUPTIBLE}.
The Linux scheduler now dispatches another task $T$ on $C_1$ to maximize CPU utilization.
(Note that $T$ is not a $K_i$ of our application. In fact, all $K_i$ are blocked in \texttt{TODO\_dequeue\_syscall}.)
When $K_1$ finally aquires the mutex and is \texttt{TASK\_RUNNING} again, it can still only be dispatched to $C_1$ due to pinning which is necessary for user-level thread migration.
However, $C_1$ will be executing $T$, not $K_1$ which is put into $C_1$'s ready queue instead.
The misery at this point is that there may exist \textbf{another} CPU $C_2$ where a KLT $K_2$ is dequeuing ULTs in the \textbf{same stage} $S$:
if $K_2$ does not have any ULTs to execute, $K_1$ should be migrated to $C_2$ immediately when it is woken up and continue execution there, benefiting from the warm on-core caches.
But the implementation only performs thread migration when a ULT calls the stage switching API.
There is no mechanism in place to save $K_1$'s state and enqueue it to $K_2$'s incoming migration queue on wake-up.
(One might assume it is possible to enqueue $U_1$ to $K_2$ since we saved its register state on kernel entry via \texttt{pthread\_mutex\_lock}:
this is not possible because there might still be kernel code that needs to run after the mutex is aquired, before returning to $U_1$.)

TODO UML sequence diagram visualizing the pathological case described above.

\section{Analysis}
Description of the implementation and and analysis of the problems observed in the user-space solution on Linux:
\begin{itemize}
    \item Cache miss rate observably reduced
    \item However, net performance is worse
    \item Reason 1: Performance of thread-migration / syscall overhead?
    \item More important resaon2: policy is not work-conserving:
        user-space apporach mandates thread migration must happen in userspace,
        thus threads that become runnable must wait for user space
    \item => fundamental impplementation problem 1: tight scheduler integration is required for a work-consercing solution.
    \item => fundamental implementation problem 2: stage definitions are application specific, the entire scheduling policy is application specific => does not fit into the traditional model of whole-system scheduler like Linux CFS
\end{itemize}

\clearpage

\section{Design \& Implementation}
In this section, we explain the design and implementation of our solution in the OSv library operating system.
The first subsection gives a brief introduction to the concepts of library OSes and the differentiators of OSv in this space.
The remaining subsections present the core ideas of our design and our modifications of OSv to realize them.

\subsection{Overview}
This section gives an overview of our design --- essentially a one-line summary of the contents of the remaining sub-sections plus some pretty diagarams.

\subsection{The OSv Library Operating System}
Short intro to OSv library OS:
\begin{itemize}
    \item Concept of a library OS: vm = app, no user-kernel-boundary => performance promise
    \item Linux ABI-compat
    \item Moderately sized C++ codebase, compact scheduler implementation
\end{itemize}
Idea: OSv allows for easier replacement of the entire scheduling policy and and thread migration mechanisms.
By the nature of a library OS, the changes are kept application-local, leaving fairness considerations up to the hypervisor scheduler (Linux / KVM in our case).

\subsection{Removal of OSv Scheduling Policy}
\subsection{User-Space API}
Mention the stack structure.
\subsection{Fast Thread Migration}
\subsection{Work Conservation}
\subsection{Stage-Aware Scheduling Policy}

\clearpage
\section{Evaluation}
In this section, we present the evaluation of our design.
We present our hardware setup and describe how we verify the supposed effects of our design decisions using various benchmarks.
This section is very closely related to the previous section.
\subsection{Evaluation Setup}
\subsection{Fast Thread Migration}
\subsection{Work Conservation}
\subsection{Stage-Aware Scheduling Policy}

\section{Conclusion}
\subsection{Future Work}
\begin{itemize}
    \item at ITEC OS Group: automatic profiling \& finding of migration points.
    \item auto-evaluating scheduler: measure if stage migrations actually make sense by computing a break-even point and continously measuring the result of scheduling decisions using performance conuters.
    \item NUMA / SMT-aware Scheduling Policy
    \item MWAIT
\end{itemize}

\section{Appendix}

\begin{itemize}
    \item Source code and commit history of our modified version of OSv
    \item Source code and commit history of our modified version of MySQL 5.6
    \item Source code and commit history of our microbenchmarks and measurement scripts
\end{itemize}

\clearpage

\printbibliography

\end{document}
