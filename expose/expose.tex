\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[inline]{enumitem}
\usepackage{parskip} % disable indentation for new paragraphs, increased margin-bottom instead
\usepackage[english]{babel}

\usepackage{hyperref}

\usepackage{todonotes}

\title{{\large Exposé for Bachelor Thesis}\\Stage-aware Scheduling in a Library OS}
\author{Christian Schwarz}
\date{October 2017}

\begin{document}

\maketitle

\section{Introduction}

Network servers account for a significant percentage of contemporary application software%
\begin{itemize*}[label={}, before={{:}}, itemjoin={{,}}]
    \item communication services (email, exchange, etc)
    \item reverse proxies
    \item message-queue systems
    \item and most importantly web-, application- and database-servers
\end{itemize*}
form the backbone of both public SaaS providers as well as corporate in-house infrastructure. 

The broad availability of cloud computing has popularized multi-tier application architectures:
the \textit{microservice} pattern encourages composition of small, scale-out services through RPC protocols.
An application component's interface is no longer defined in a programming language but merely consists of its published
RPC endpoints.
Applications backed by a database server are a prime exmaple for this kind of deployment:
the database -- being a typical bottleneck -- is usually the first to be installed on a dedicated machine.

The dominant CPUs in this market are \texttt{x86\_64} based Intel processors targeted at the mass market and tuned for an
extreme variety of workloads.
Hardware-supported operating system virtualization is used to isolate applications both for simplified administration
and increased security.
Linux is the dominant guest operating system offering a familiar environment and high-performance paravirtualization drivers.

% explain how applications are structured
The server applications running on top of this stack are primarily concerned with \emph{concurrent} request handling.
A common design pattern is to implement a sequential \textit{handler} routine to process each request, consisting of
logical sections such as protocol handling, request decoding, the actual business logic, response encoding, etc.
Concurrency is then achieved by handling each request in separate operating system thread -- either
by actually spawning a thread per request or by using a thread pool.
The sequential handler implementation is easy to understand and extend and
abstractions for the aforementioned threading models are available in virtually any programming language.
While more recent developments in language \& library design challenge this approach\footnote{coroutines, async IO, Node.js, Go},
many legacy applications still follow this model, most notably popular database systems such as MySQL.

Recent large-scale profiling at Google \cite{kanev2015profiling} suggests that the memory hierarchy of commodity
processors is suboptimal for contemporary applications:
the on-core cache size is too small to fit the working sets of the executed threads.
This \textit{cache thrashing} results in degraded performance and increases energy consumption, which is undesirable
regardless of the scale of deployment.

In section \ref{relatedwork} of this exposé, we examine previous work in this field and
identify aspects of contemporary computer architecture, software enginerring (application development specifically)
and realities of software deployment as contributors to the observed suboptimal resource utilization in today's datacenters.

Leaning on the idea of \emph{staged architectures} in DBMS design \cite{seda} dating back to the early 2000s,
we propose an operating system based solution to exploit intra-query cache locality in server applications.
A key differentiator to exisiting approaches is that only minimal changes to existing application code are required.
This makes our proposal particularly attractive for large legacy code bases that cannot be trivially refactored to a
truly staged architecture as proposed in \cite{seda}.

\section{Background \& Related Work}\label{relatedwork}

As mentioned before, the idea of staged computation dates back to the early 2000s:
\cite{seda} makes the case for \textit{staged event driven architectures} (SEDA) for network servers and
develop a Java-based implementation framework.
\cite{harizopoulos2003case} and \cite{harizopoulos2005staged} focus on database systems, identify I-cache
misses as the primary bottleneck and implement a staged architecture in a research database system.
Cohort scheduling \cite{cohort} applies the idea of staged execution to scheduling policies and proposes
a corresponding software framework: request handlers about to enter the next stage are preempted, grouped into
\textit{cohorts} and then -- for one stage -- dispatched consecutively on the same processor.
Request latency is thus improved if the gain of warmed-up I-caches outweighs the time lost in the barrier-like batching
of threads into a cohort.

All of the aforementioned papers lean heavily on event-driven designs, with prototype implementations thar are
system or language specific.
\emph{Software data spreading} \cite{sodaspr} approaches cache thrashing from a different angle: compiler-profiling is used
to identify code snippets with a \emph{data} working set greater than that of a single processor core.
The compiler partitions the snippet's working set into sub-working sets smaller than a single on-core cache, maps each
sub-working set to a different core and generates thread migration code whenver the sub-working set changes.
Even if the sub-working sets are not totally disjoint, the authors still claim performance gains
on chips with built-in cache-to-cache transfer functionality.
Solutions for cases where the working set exceeds the sum of all on-core cache sizes are also covered.
However, it should be noted that \cite{kanev2015profiling} re-emphasizes that for \textit{warehouse-scale computing} applications
like those measured at Google, \emph{instruction} caches are the primary bottleneck in the memory hierarchy.
The software data spreading paper does not address this issue.

\subsection{Proof of Concept at the KIT OS Group}\label{proofofconcept}

A Linux-based proof of concept implementation at the KIT OS group \todo{ok?} combines the idea of staged execution, cohort
scheduling and software data spreading into a C API that allows for intuitive conversion of existing code bases to
staged execution:
The developer manually identifies stages and inserts library calls into application code for switching the current stage.
Each CPU core is assigned one or more stages and each time a thread switches to a new stage, it is migrated to a core
assigned to that stage.
%While this sounds cumbersome, converting request handlers is matter of minutes given some insight on the instruction
%footprint between high-level library calls, which can be easily derived from stack sampling and CPU performance
%counters.

\emph{Thread migration} thus must be fast.
%\newcommand{\setaffinity}{\texttt{sched\_setaffinity(2)}}
The built-in facility for this purpose, \texttt{sched\_setaffinity}, is impractical because Linux uses
expensive inter-processor-interrupts to implement this syscall.
The proof of concept thus opted for a hybrid implementation in both user and kernel space resembling an M:N threading model:
kernel-level-threads (KLTs) are pinned to a specific core and stage.
User-level threads (ULTs) as seen by the application are always in exactly one stage and run on a KLT.
Note that there is still one KLT per ULT but a ULT can be migrated to a different KLT when switching stages.

When a ULT makes a call to switch stages, the context of the ULT is saved and enqueued to the next stage's ready queue.
The KLT that executed that ULT now waits for new ULTs on its own stage's ready queue.
If it is empty, the KLT makes a blocking syscall to a kernel component to wait for work, avoiding busy waiting
and enabling switching of its assigned stage through the return value of the call.
The kernel component is also used to track KLT state changes by callback from the scheduler, ensuring there is always at
least one KLT per stage actively dequeuing ULTs.
If KLT state changes were not tracked, a ULT blocking on a mutex would render its KLT
\texttt{TASK\_INTERRUPTIBLE} and no other ULTs would be dequeued until the mutex is aquired and the KLT switches back to
\texttt{TASK\_RUNNING}.

The main flaw of the implementation shows on multiprocessor systems with $\frac{\#cores}{stage} > 1$.
A ULT $U_1$ executes on a KLT $K_1$ and $K_1$ is pinned to core $C_1$.
When $U_1$ performs a blocking syscall, e.g. waiting for a mutex, $K_1$ blocks and becomes \texttt{TASK\_INTERRUPTIBLE}.
The Linux scheduler now dispatches another task on $C_1$ to maximize resource utilization.
When $K_1$ finally aquires the mutex and is \texttt{TASK\_RUNNING} again, it can still only be dispatched to $C_1$ due to
pinning.
However, another idle KLT $K_2$ on a different core but same stage as $K_1$ could in theory continue $U_1$.
But the implementation only performs thread migration when a ULT calls the stage switching API.
The net effect is CPU underutilization -- the structure of the implementation is not work conserving.

\section{Proposed Solution}\label{propsolution}

We identify a core architectural problem in the design of the proof of concept implementation:
the user-level code cannot react to thread state changes until the thread is rescheduled and the user-level part of the
implementation is invoked through a call from the ULT. A architecturally clean solution requires tight integration with
the scheduler since staged scheduling is effectively a scheduling policy.

Scheduler activations promise \cite{schedulerActivations} a general solution for user-space directed scheduling.
However, practical implementations of scheduler activations in production operating systems have since been
deprecated or removed in favor of 1:1 threading models\cite{todo}.
On the opposite end, a clean implementation in Linux would need to replace the current \emph{completely fair scheduler} (CFS).
CFS is a system-wide scheduling policy with significant amounts put into it.
Staged scheduling as proposed in the proof of concept and pursued by this thesis is an application-local policy.
Replacing the Linux CFS implementation with full in-kernel staged scheduling is thus not only a daring
endeavor implementation-wise but simply an unsuitable choice.

Given above considerations and experiences, we therefore propose an implementation of staged scheduling in the OSv
library operating system.
This leaves system-wide scheduling to the hypervisor OS and allows for tight integration and customization of the OSv scheduler.
The nonexistence of a user-kernel-boundary promises a low overhead "in-kernel" implementation of per-stage queueing,
moving everything but the stage switch syscall out of the application,
making our solution language-agnostic and only dependent on the per-request threading model.

The questions to be answered by this thesis are:
\begin{enumerate}
    \item Can we implement a robust staged scheduling policy in OSv?
    \item Is the scheduler work-conserving, especially with regards to task state changes described in section \ref{proofofconcept}?
    \item How does our implementation compare to Linux and unmodified OSv? Specifically...
        \begin{enumerate}
            \item Can we observe increased net-performance in applications with large instruction footprints?
            \item Can we identify reduced I-cache misses with our scheduling policy?
        \end{enumerate}
\end{enumerate}

\subsection{Work Plan}

The coarse work plan for this thesis lays out as follows:

% The concrete work plan lays out as follows:
\begin{enumerate}
    \item \label{stageapi} Define an system interface allowing an application to communicate stage switches to OSv.
            A corresponding API should be provided for C and C++ programs.
    \item \label{schedpolicy} Implement a \textit{stage-aware scheduling policy} in OSv
            using information from the aforementioned interface (see section \ref{stageawaresched}).
    \item \label{microbench} Develop a suite of microbenchmarks to measure the effect of our implementation on the memory hierarchy.
        The benchmarks should mimic the behavior observed in \cite{kanev2015profiling}, namely I-cache and D-cache capacity misses.
        Furthermore, benchmarks resembling a microservice performing RPC to various backends are of special interest.
    \item Evaluate the scheduler with microbenchmarks and reiterate on its implementation as necessary.
    \item \label{appdesign} Find an existing server application $A$ written in C or C++ that
    \begin{enumerate}
        \item handles requests by spawning or re-using an operating system thread,
        \item keeps this relationship between thread and request intact until the request handler has finished executing,
        \item \label{app_osv_image} can be compiled and built into an OSv virtual machine image.
    \end{enumerate}
    \item Customize $A$'s code to use the stage API, based on developer experience, intuition or arbitrary profiling.
    \item Compare the net performance metrics (black box testing) of $A$ running on stage-aware OSv to unmodified OSv and Linux.
        Reiterate on the customization of $A$ as necessary and document the process of experimenting with the stage API in a production codebase.
\end{enumerate}

Some work has already been started for the proof of concept implementation:
a microbenchmark framework for memory hierarchy access, a C-API for stage switching and customizations to the MySQL
server already exist.
MySQL is an excellent demo application because databases are typical bottlenecks in web apps and usually the
first to be installed on a dedicated machine. It implements the required threading model \cite{mysqlThreading},
and has already been adopted to the stage API for evaluation of the proof of concept.

\subsubsection{Stage-Aware Scheduling}\label{stageawaresched}

The development of a stage-aware scheduling policy and its evaluation is the core task of this thesis.
The remainder of this subsection outlines the planned implementation followed by open issues.

The basic idea behind our proposal is to leverage knowledge about a thread's current stage when making scheduling decisions:
\begin{quote} %TODO right env
A ready thread in stage $S_i$ should be scheduled on a processor $P$ that last ran a thread also in stage $S_i$.
\end{quote}

To maintain tractability of this problem, we assume a fixed number of $P$ symmetric processors with a uniform memory architecture.
NUMA architectures do not receive special attention in this thesis. \todo{justify this?}% TODO justify? basically, the NUMA costs on thread migration must be taken into account... what else?

At the time of writing this exposé, the implementation details have not been figured out yet.
Thus, the remainder of this section presents aspects that should be kept in mind when designing and implementing the scheduling policy:

The aforementioned \textit{stage C API} is effectively an additional entry point to the OSv scheduler.

By scheduling a stage on the same core, the pre-warmed cache and
the assumed correct partitioning of the thread's stages
will lead to a reduction in cache misses and thus better performance.
It should be apparent that this idea borrows heavily from \cite{sodaspr,cohort}.

Recall variables \textbf{$S$} (all stages), \textbf{$P$} (all processors) and \textbf{$T$} (\#threads):\\
If $|S| \le |P|$, a first draft of the scheduling policy would
assign each stage $S_i$ to a single $P_i$ and schedule threads in $S_i$ only the corresponding $P_i$.

By spreading a thread's distinct working sets over $P$ processors,
the effectively available cache size is increased up to a factor of $|P|$, depending on
\begin{enumerate*}
    \item how disjoint the stage's working sets actually are and
    \item that the size of each stage's working set is smaller than the on-core cache size ($size(S_i) \overset{!}{\le} size{on\_core\_cache}$)
\end{enumerate*}.

However, if $S > P$
\begin{itemize}
    \item \textit{victim stages} must be found that are a scheduled on the same processor,
    \item the effectiveness of the choice of victim stages could be evaluated using CPU performance counters and
    \item given this knowledge, other victim stages could be chosen.  
\end{itemize}

Apart from the mapping of $S \mapsto P$, it is an open question how well the proposed technique scales with the number of stages and each stages working set size $size(S_i)$.
We assume there exists a break-even value for $size(S_i)$ below which migration costs exceed the benefits reaped from the proposed policy.
% (Working sets will not be $100\%$ distinct, cache misses will occur, cache coherence protocol costs, etc). 

Also, the proposed 1:1 mapping $S \mapsto P$ leads to suboptimal resouce utilization if any stage $S_c$ is a point of congestion:
threads will compete for the processor $P_c$ and under-utilize all other processors $P_d$ where $d \ne c$.

An obvious solution is to assign $S_c$ to several $P_{c,i}$, introducing another scheduling dimension.
However, this still only works if the average stage execution times are of the same order of magnitude.

A last aspect are threads not spawned by the application but by OSv (traditionally called \emph{kernel-only threads}),
e.g. file system worker threads.\todo{make it clear we don't mean the upper half of FS code executed by a syscall}
Ideally, these should also be adapted to the staging API although we figure time will not allow for this to happen.

We can conclude that the simple stage-aware scheduling policy outlined above is rather specific and
is likely suboptimal for a multi-purpose / multi-user operating system.

With OSv though, we are targeting a library OS:
the entire system is running a single server app and all resources dedicated to the virtual machine are available for that purpose.

\subsection{Evaluation}\label{evaluation}

\subsubsection{Environment}\label{environment}
Before discussing the details of how we evaluate our implementation,
we need to investigate how the usage of a library OS constraints our benchmarking environment:

\begin{itemize}
    \item The proposed scheduling policy explicitly targets server applications.
    \item A benchmark for a server application does not behave like a server application.
    \item Thus, executing the benchmark \emph{inside} the OSv VM means that we
    \begin{enumerate}
        \item would need to adapt the benchmark to the stage API or
        \item would need a special-case in the scheduler for the benchmark.
    \end{enumerate}
    \item Either way, distorted benchmark results are to be expected.
\end{itemize}

The virtual machines used for the benchmarks are configures as follows:
\begin{enumerate}
    \item Unmodified OSv + unmodified application
    \item Stage-aware OSv + modified application
    \item \label{linuxvmsetup} Linux-based VM + unmodified application
\end{enumerate}
The hypervisor is planned to be Linux KVM using \texttt{virtio-*} paravirtualization whenever possible.

We recognize that running the Linux setup in a VM may not be a fair comparison:
OSv \emph{requires} execution in a virtualized environment due to lack of drivers
but Linux could also use the host's hardware directly (and use cpu\_affinity settings for comparable benchmarks).\\
If performance gains in OSv do not significantly exceed the expected virtualization overhead, this aspect should be reconsidered.

Lastly, we want to eliminate differences in the Disk IO subsystems from the benchmarks:
in both the OSv and Linux VMs, the file system storing database data must be RAM-based.
\todo{Alternatively: Since OSv only has ZFS as alternative, run ZFS on LInux inside linux vm?}

\subsubsection{Benchmarks}

Given the environment outlined above, we propose to execute \textbf{black-box benchmarks} against the virtual machines.
The primary success metrics are
\begin{itemize}
    \item request latency
    \item total request duration
    \item standard deviation of the above.
\end{itemize}

These metrics are suitable to measure the practical advantage of our implementation but
do not validate our assumption that performance gains result from improved cache hit/miss ratio.

We plan to use performance counters to measure cache hit/miss ration during the benchmark.
However, further investigation is required to decide whether this should be implemented on the hypervisor or in the guest OSes themselves
and how this data is merged with the benchmark results.

\subsubsection{MySQL Benchmarks}

As outlined in the working plan, the first application to be adapted to stage-aware scheduling is the MySQL database server.

The \textit{OLTPBenchmark} project (see (\cite{oltpbench})) combines several RDBM benchmarks.
\todo{which exactly? should this be mentioned here?}
Unfortunately, most are designed to be executed on the same machine as the database server, which does not work with OSv (see \ref{environment}).
Thus, we expect the need to customize the benchmark code with regards to this issue.

\subsubsection{Synthetic Application}

In addition applications such as MySQL, a less complex, synthetic application seems appropriate.

\subsubsection{Other Applications}

It should be noted that if time allows for it, other applications with readily usable benchmarks could be ported.

\clearpage
\bibliographystyle{alpha}
\bibliography{bib.bib}

\end{document}
