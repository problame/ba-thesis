\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[inline]{enumitem}
\usepackage{parskip} % disable indentation for new paragraphs, increased margin-bottom instead
\usepackage[english]{babel}

\usepackage{hyperref}

\usepackage{todonotes}

\title{{\large Exposé for Bachelor Thesis}\\Stage-aware Scheduling in a Library OS}
\author{Christian Schwarz}
\date{October 2017}

\begin{document}

\maketitle

\section{Introduction}

Network servers account for a significant percentage of contemporary application software%
\begin{itemize*}[label={}, before={{:}}, itemjoin={{,}}]
    \item communication services (email, exchange, etc)
    \item reverse proxies
    \item message-queue systems
    \item and most importantly web-, application- and database-servers
\end{itemize*}
form the backbone of both public SaaS providers as well as corporate in-house infrastructure. 

The broad availability of cloud computing has popularized multi-tier application architectures:
the \textit{microservice} pattern encourages composition of small, scale-out services through RPC protocols.
An application component's interface is no longer defined in a programming language but merely consists of its published
RPC endpoints.
Applications backed by a database server are a prime exmaple for this kind of deployment:
the database -- being a typical bottleneck -- is usually the first to be installed on a dedicated machine.

The dominant CPUs in this market are \texttt{x86\_64} based Intel processors targeted at the mass market and tuned for an
extreme variety of workloads.
Hardware-supported operating system virtualization is used to isolate applications both for simplified administration
and increased security.
Linux is the dominant guest operating system offering a familiar environment and high-performance paravirtualization drivers.

% explain how applications are structured
The server applications running on top of this stack are primarily concerned with \emph{concurrent} request handling.
A common design pattern is to implement a sequential \textit{handler} routine to process each request, consisting of
logical sections such as protocol handling, request decoding, the actual business logic, response encoding, etc.
Concurrency is then achieved by handling each request in separate operating system thread -- either
by actually spawning a thread per request or by using a thread pool.
The sequential handler implementation is easy to understand and extend and
abstractions for the aforementioned threading models are available in virtually any programming language.
While more recent developments in language \& library design challenge this approach\footnote{coroutines, async IO, Node.js, Go},
many legacy applications still follow this model, most notably popular database systems such as MySQL.

Recent large-scale profiling at Google \cite{kanev2015profiling} suggests that the memory hierarchy of commodity
processors is suboptimal for contemporary applications:
the on-core cache size is too small to fit the working sets of the executed threads.
This \textit{cache thrashing} results in degraded performance and increases energy consumption, which is undesirable
regardless of the scale of deployment.

In section \ref{relatedwork} of this exposé, we examine previous work in this field and
identify aspects of contemporary computer architecture, software enginerring (application development specifically)
and realities of software deployment as contributors to the observed suboptimal resource utilization in today's datacenters.

Leaning on the idea of \emph{staged architectures} in DBMS design \cite{seda} dating back to the early 2000s,
we propose an operating system based solution to exploit intra-query cache locality in server applications.
A key differentiator to exisiting approaches is that only minimal changes to existing application code are required.
This makes our proposal particularly attractive for large legacy code bases that cannot be trivially refactored to a
truly staged architecture as proposed in \cite{seda}.

\section{Background \& Related Work}\label{relatedwork}

As mentioned before, the idea of staged computation dates back to the early 2000s:
\cite{seda} makes the case for \textit{staged event driven architectures} (SEDA) for network servers and
develop a Java-based implementation framework.
\cite{harizopoulos2003case} and \cite{harizopoulos2005staged} focus on database systems, identify I-cache
misses as the primary bottleneck and implement a staged architecture in a research database system.
Cohort scheduling \cite{cohort} applies the idea of staged execution to scheduling policies and proposes
a corresponding software framework: request handlers about to enter the next stage are preempted, grouped into
\textit{cohorts} and then -- for one stage -- dispatched consecutively on the same processor.
Request latency is thus improved if the gain of warmed-up I-caches outweighs the time lost in the barrier-like batching
of threads into a cohort.

All of the aforementioned papers lean heavily on event-driven designs, with prototype implementations thar are
system or language specific.
\emph{Software data spreading} \cite{sodaspr} approaches cache thrashing from a different angle: compiler-profiling is used
to identify code snippets with a \emph{data} working set greater than that of a single processor core.
The compiler partitions the snippet's working set into sub-working sets smaller than a single on-core cache, maps each
sub-working set to a different core and generates thread migration code whenver the sub-working set changes.
Even if the sub-working sets are not totally disjoint, the authors still claim performance gains
on chips with built-in cache-to-cache transfer functionality.
Solutions for cases where the working set exceeds the sum of all on-core cache sizes are also covered.
However, it should be noted that \cite{kanev2015profiling} re-emphasizes that for \textit{warehouse-scale computing} applications
like those measured at Google, \emph{instruction} caches are the primary bottleneck in the memory hierarchy.
The software data spreading paper does not address this issue.

\subsection{Proof of Concept at the KIT OS Group}\label{proofofconcept}

A Linux-based proof of concept implementation at the KIT OS group \todo{ok?} combines the idea of staged execution, cohort
scheduling and software data spreading into a C API that allows for intuitive conversion of existing code bases to
staged execution:
The developer manually identifies stages and inserts library calls into application code for switching the current stage.
Each CPU core is assigned one or more stages and each time a thread switches to a new stage, it is migrated to a core
assigned to that stage.
%While this sounds cumbersome, converting request handlers is matter of minutes given some insight on the instruction
%footprint between high-level library calls, which can be easily derived from stack sampling and CPU performance
%counters.

\emph{Thread migration} thus must be fast.
%\newcommand{\setaffinity}{\texttt{sched\_setaffinity(2)}}
The built-in facility for this purpose, \texttt{sched\_setaffinity}, is impractical because Linux uses
expensive inter-processor-interrupts to implement this syscall.
The proof of concept thus opted for a hybrid implementation in both user and kernel space resembling an M:N threading model:
kernel-level-threads (KLTs) are pinned to a specific core and stage.
User-level threads (ULTs) as seen by the application are always in exactly one stage and run on a KLT.
Note that there is still one KLT per ULT but a ULT can be migrated to a different KLT when switching stages.

When a ULT makes a call to switch stages, the context of the ULT is saved and enqueued to the next stage's ready queue.
The KLT that executed that ULT now waits for new ULTs on its own stage's ready queue.
If it is empty, the KLT makes a blocking syscall to a kernel component to wait for work, avoiding busy waiting
and enabling switching of its assigned stage through the return value of the call.
The kernel component is also used to track KLT state changes by callback from the scheduler, ensuring there is always at
least one KLT per stage actively dequeuing ULTs.
If KLT state changes were not tracked, a ULT blocking on a mutex would render its KLT
\texttt{TASK\_INTERRUPTIBLE} and no other ULTs would be dequeued until the mutex is aquired and the KLT switches back to
\texttt{TASK\_RUNNING}.

The main flaw of the implementation shows on multiprocessor systems with $\frac{\#cores}{stage} > 1$.
A ULT $U_1$ executes on a KLT $K_1$ and $K_1$ is pinned to core $C_1$.
When $U_1$ performs a blocking syscall, e.g. waiting for a mutex, $K_1$ blocks and becomes \texttt{TASK\_INTERRUPTIBLE}.
The Linux scheduler now dispatches another task on $C_1$ to maximize resource utilization.
When $K_1$ finally aquires the mutex and is \texttt{TASK\_RUNNING} again, it can still only be dispatched to $C_1$ due to
pinning.
However, another idle KLT $K_2$ on a different core but same stage as $K_1$ could in theory continue $U_1$.
But the implementation only performs thread migration when a ULT calls the stage switching API.
The net effect is CPU underutilization -- the structure of the implementation is not work conserving.

\section{Proposed Solution}\label{propsolution}

We identify a core architectural problem in the design of the proof of concept implementation:
the user-level code cannot react to thread state changes until the thread is rescheduled and the user-level part of the
implementation is invoked through a call from the ULT. A architecturally clean solution requires tight integration with
the scheduler since staged scheduling is effectively a scheduling policy.

Scheduler activations promise \cite{schedulerActivations} a general solution for user-space directed scheduling.
However, practical implementations of scheduler activations in production operating systems have since been
deprecated or removed in favor of 1:1 threading models\cite{todo}.
On the opposite end, a clean implementation in Linux would need to replace the current \emph{completely fair scheduler} (CFS).
CFS is a system-wide scheduling policy with significant amounts put into it.
Staged scheduling as proposed in the proof of concept and pursued by this thesis is an application-local policy.
Replacing the Linux CFS implementation with full in-kernel staged scheduling is thus not only a daring
endeavor implementation-wise but simply an unsuitable choice.

Given above considerations and experiences, we therefore propose an implementation of staged scheduling in the OSv
library operating system.
This leaves system-wide scheduling to the hypervisor OS and allows for tight integration and customization of the OSv scheduler.
The nonexistence of a user-kernel-boundary promises a low overhead "in-kernel" implementation of per-stage queueing,
moving everything but the stage switch syscall out of the application,
making our solution language-agnostic and only dependent on the per-request threading model.

The questions to be answered by this thesis are:
\begin{enumerate}
    \item Can we implement a robust staged scheduling policy in OSv?
    \item Is the scheduler work-conserving, especially with regards to task state changes described in section \ref{proofofconcept}?
    \item How does our implementation compare to Linux and unmodified OSv? Specifically...
        \begin{enumerate}
            \item Can we observe increased net-performance in applications with large instruction footprints?
            \item Can we identify reduced I-cache misses with our scheduling policy?
        \end{enumerate}
\end{enumerate}

\subsection{Work Plan}

The coarse work plan for this thesis lays out as follows:

% The concrete work plan lays out as follows:
\begin{enumerate}
    \item \label{stageapi} Define an system interface allowing an application to communicate stage switches to OSv.
            A corresponding API should be provided for C and C++ programs.
    \item \label{schedpolicy} Implement a \textit{stage-aware scheduling policy} in OSv
            using information from the aforementioned interface (see section \ref{stageawaresched}).
    \item \label{microbench} Develop a suite of microbenchmarks to measure the effect of our implementation on the memory hierarchy.
        The benchmarks should mimic the behavior observed in \cite{kanev2015profiling}, namely I-cache and D-cache capacity misses.
        Furthermore, benchmarks resembling a microservice performing RPC to various backends are of special interest.
    \item Evaluate the scheduler with microbenchmarks and reiterate on its implementation as necessary.
    \item \label{appdesign} Find an existing server application $A$ written in C or C++ that
    \begin{enumerate}
        \item handles requests by spawning or re-using an operating system thread,
        \item keeps this relationship between thread and request intact until the request handler has finished executing,
        \item \label{app_osv_image} can be compiled and built into an OSv virtual machine image.
    \end{enumerate}
    \item Customize $A$'s code to use the stage API, based on developer experience, intuition or arbitrary profiling.
    \item Compare the net performance metrics (black box testing) of $A$ running on stage-aware OSv to unmodified OSv and Linux.
        Reiterate on the customization of $A$ as necessary and document the process of experimenting with the stage API in a production codebase.
\end{enumerate}

Some work has already been started for the proof of concept implementation:
a microbenchmark framework for memory hierarchy access, a C-API for stage switching and customizations to the MySQL
server already exist.
MySQL is an excellent demo application because databases are typical bottlenecks in web apps and usually the
first to be installed on a dedicated machine. It implements the required threading model \cite{mysqlThreading},
and has already been adopted to the stage API for evaluation of the proof of concept.

\subsubsection{Stage-Aware Scheduling}\label{stageawaresched}

The development of a stage-aware scheduling policy and its evaluation is the core task of this thesis.
The remainder of this subsection outlines the planned implementation followed by open issues.

The basic idea behind our proposal is to leverage knowledge about a thread's current stage when making scheduling decisions:
\begin{quote} %TODO right env
A ready thread in stage $S_i$ should be scheduled on a processor $P$ that last ran a thread also in stage $S_i$.
\end{quote}

By scheduling a stage on the same core, the pre-warmed on-core caches and the correct partitioning of the thread's stages
will lead to a reduction in cache misses and thus better performance.
The aforementioned stage API is effectively an additional entry point to the OSv scheduler and can be thought of as an
extended \texttt{sched\_yield(2)}.
It should be apparent that this idea borrows heavily from \cite{sodaspr,cohort} and \cite{gottschlag2017}.

To maintain tractability of this problem, we assume a fixed number of $P$ symmetric processors with a uniform memory architecture.
Expansion our approach to NUMA systems is left open for further research.

We define the following variables to reason about details of the scheduling policy:
{$\mathbf{S}$} (all stages), $\mathbf{P}$ (all processors) and $\mathbf{T}$ (\#threads).
If $|S| \le |P|$, a simple idea is to assign each $P_i$ exactly one $S_i$ and schedule threads in $S_i$ only on the corresponding $P_i$.
By spreading a thread's sub-working sets over $|P|$ processors, the effectively available cache size is thus increased up to a factor
of $|P|$. We expect a high degree of disjointness in text working sets (instructions) by simply partioning code paths
using the stage API.

If $|S| > |P|$ however, stages must necessarily share a processor, limiting the effect of partitioning.
Naturally, some stages must be selected as \textit{victims} but the precise algorithms for doing so remains open:
possible options range from developer-indicated priorities over contention metrics such as ready-list length
to auto-evaluation using CPU performance counters.
Additionally, we expect interference with SMT support (\textit{hyperthreading}) in both implementation and evaluation.

Apart from the mapping of $S \mapsto P$, it is an open question how well the proposed technique scales with the number
of stages and each stage's working set size $size(S_i)$.
We assume there exists a break-even value for $size(S_i)$ below which migration costs exceed the benefits reaped from the proposed policy.
% (Working sets will not be $100\%$ distinct, cache misses will occur, cache coherence protocol costs, etc). 

Also, the proposed 1:1 mapping $S \mapsto P$ leads to suboptimal resouce utilization if any stage $S_c$ is a point of congestion:
threads will compete for the processor $P_c$ and under-utilize all other processors $P_d$ where $d \ne c$.
An obvious solution is to assign $S_c$ to several $P_{c,i}$, introducing another scheduling dimension.
However, this still only works if the average stage execution times are of the same order of magnitude.

A last aspect are threads not spawned by the application but by OSv (traditionally called \emph{kernel-only threads}),
e.g. file system worker threads.
Ideally, these should also be adapted to the staging API although we figure time will not allow for this to happen.

We can conclude that the simple stage-aware scheduling policy outlined above is rather specific and
is likely suboptimal for a multi-purpose / multi-user operating system.
With OSv though, we are targeting a library OS:
the entire system is running a single server app and all resources dedicated to the virtual machine are available for that purpose.

\section{Evaluation}\label{evaluation}

Stage-aware scheduling is based on the idea that executing a thread in a particular stage on a processor dedicated to
that stage improves instruction and cache locality and thus application performance.
The evaluation of our implementation must therefore verify that
\begin{enumerate*}[label=\emph{(\alph*)}]
    \item this pre-warming effect on the on-core caches actually exists and
    \item that our technique actually leads to better net application performance.
\end{enumerate*}

Performance counters in contemporary Intel CPUs offer rich instrumentation of the memory hierarchy. They can be used as a
data source for verifying reduced on-core cache misses and are applicable to any type of application.
In contrast, measuring net performance gain is application-specific and requires domain-specific benchmarks.
An application benchmark creates a typical workload by sending a realistic mix of $\mathbf{N}$ parallel requests for a
total time $\mathbf{T_{bench}}$ and recording a set of quality metrics per request.
The resulting dataset allows for extraction of the following aggregated metrics per value of $\mathbf{N}$:
\begin{enumerate}[label=\emph{(\alph*)}]
    \item achieved throughput [requests/sec]
    \item average / min / max request latency [sec]
    \item $n^{th}$ percentile of request latency [sec]
\end{enumerate}
\todo{still too vague / too few? Playing with OLTPBench and HammerDB for a few hours didn't yield more ideas...}
Additionally, for fixed $N$, the development of these metrics over the course of $T_{bench}$ is useful for identifying
application-level warm-up effects, internal caching, etc.

\todo{mention that SMT enabled yesno should be recorded?}

\subsection{Bechmarking Environment \& Candidates}\label{environment}

Our stage-aware scheduling solution is built into OSv where applications and "OS" code are bundled into an opaque virtual
machine image.
We explicitly target server applications but the corresponding benchmarks do not necessarily exhibit the same runtime
behavior.
Thus, executing benchmarks \emph{inside} the OSv VM means that we would either need to adapt the benchmarking code to
use the staging API or need a special-case in the scheduler.
Neither of these options is desirable and thus makes black-box benchmarking from the hypervisor an attractive choice.
At the time of writing we still need to investigated whether performance counters can also be read from the hypervisor
and cleanly attributed to a virtual machine.

The three test setups to be compared shall be configured as follows:
\begin{enumerate}
    \item Unmodified OSv + unmodified application,
    \item Stage-aware OSv + modified application,
    \item \label{linuxvmsetup} Linux-based VM + unmodified application.
\end{enumerate}
The hypervisor is planned to be Linux KVM using \texttt{virtio-*} paravirtualization whenever possible.

We recognize that running the Linux setup in a VM may not be a fair comparison:
OSv \emph{requires} execution in a virtualized environment due to lack of drivers
but Linux could also use the host's hardware \emph{directly} (and use \texttt{taskset(1)} for comparable benchmarks).
If performance gains in OSv do not significantly exceed the expected virtualization overhead, this aspect should be reconsidered.

Lastly, differences in the disk IO subsystems should be eliminated from the benchmarks:
in both the OSv and Linux VMs, the file system storing database data must be RAM-based.
It should be considered whether the cache footprint of the in-memory filesystem conflicts with our scheduling policy
and if so whether memory used for it can be mapped with caching disabled.

\subsubsection{Microbenchmarks \& Synthetic Applications}

As explained in work plan item \ref{microbench}, the the staging API shall be first tested with a set of synthetic
applications.
Some of these may not be synthetic server applications but merely microbenchmarks exhibiting certain memory access patterns
with interleaved stage API calls.
In this case the result data set would reduce to metrics extracted from the memory hierarchy.
Work in this direction has already been done in the KIT OS group's proof of concept implementation
(see section \ref{proofofconcept}).


\subsubsection{MySQL Benchmarks}

The MySQL database server fits our required threading model and has already been adapted to the staging API
(see section \ref{proofofconcept}).
The \textit{OLTPBenchmark} project \cite{oltpbench} combines several RDBM benchmarks that simulate different kinds of
realistic transaction workloads on an SQL database server.
Our generic quality metrics are exported in the CSV format and the configuration format allows for reproducible test
setups.
All benchmarks work over TCP and can thus be executed from the hypervisor, outside of OSv or virtualized Linux.


\clearpage
\bibliographystyle{alpha}
\bibliography{bib.bib}

\end{document}
